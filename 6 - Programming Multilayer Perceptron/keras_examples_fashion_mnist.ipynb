{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, Flatten, MaxPooling2D,  Activation\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from PIL import Image\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fashion, 10 classes, 28x28 images \n",
    "ds = load_dataset(\"zalando-datasets/fashion_mnist\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ds['train'].with_format(\"np\")\n",
    "test_ds = ds['test'].with_format(\"np\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set:  Dataset({\n",
      "    features: ['image', 'label'],\n",
      "    num_rows: 60000\n",
      "})\n",
      "test set:  Dataset({\n",
      "    features: ['image', 'label'],\n",
      "    num_rows: 10000\n",
      "})\n",
      "\n",
      " label of image:  9\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARgAAAEYCAAAAACi5bZQAAAEXklEQVR4nO3dv65UZRiF8cNxEBAQDKIgiWCBBgmhMojGzkhvaGy14QK4CVpvhIQCG6+ABmJFYqMBQySAfwD1gILdejB51+QkJ+EQ8/yqlWH2nn3W7OLNl282KyuSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmStBFbNvsCNualpCdJT4f3bUtaSzqa9P1wxOqGrut/zGIKiyksprCYwmIKiyksprCYYrHZF/Ds8E1ijj2UdDrpm6SH6/yMteG1z5MuDP/qHVNYTGExhcUUFlNYTGExhcUUFlO8AJMvngyvfZJ0KumtpK/XeeY3ks4k3V96hHdMYTGFxRQWU1hMYTGFxRQWU1hM8QJMvuxY+Dvpg6RjST8nsU/hYtK9pB1JPybtS3o16ebSq/KOKSymsJjCYgqLKSymsJjCYgqLKTZt8uUbYd7dmXQ2iX0K25N2J7E/YnV47XjSjaRfkpb/6d4xhcUUFlNYTGExhcUUFlNYTGExxYYmX2ZMflW2OrxGYn33n+F855JY3/0r6XDS9uF902/d2AX8KIk1X371xsTNEd4xhcUUFlNYTGExhcUUFlNYTGExxbon32nKnZ6iMO3VXT7vfpF0IOlqEhe4N+luEnscXk9iRZjPBXfCK0nsnrg2vE//YTGFxRQWU1hMYTGFxRQWU1hMse7Jd5pyV4fEbPt0eA1fJr2bxJ4E9uUycbN/96ckplwm7j+SWBue5nbwq7drSd4xhcUUFlNYTGExhcUUFlNYTGExxTj5Tm0xMU7PHptWesGzGHiSGHMsT9PdlcROBGZgdixwLazbgjl7bXiN/Qxc88fDWbxjCospLKawmMJiCospLKawmMJiisW0E2H5HDutmu5POpL0XtLBJKbX35P2JrEHd2sSMzBXdXh4369Jj4cj+P7/TOIv5/lm/CbOO6awmMJiCospLKawmMJiCospLKZYTDsR3kxixtw5JNZt30liFZYJ9EES38Oe4Sw85YGzsHeBFdyXk24N5+NYnuPAavJrSaz+sr+YFWbvmMJiCospLKawmMJiCospLKawmOKZ3Q6fJrE7gVmUVd1plZj3sX7KtMlkyU4J1nKZT/mWOJZPY1Jlkv4tif+rYsJnsA7MxM218Hd4xxQWU1hMYTGFxRQWU1hMYTGFxRSLzxK/SrqexJoqEy1dsneB+ZTZliNYo2XunJ7Ju2N4H/sZmJ9Zk35/+Izpu2ZqZkWY56UxSd9eehatWExlMYXFFBZTWExhMYXFFBZTLK4kfph0Imn67de0vntvSKzHMpUy5bKvgP3ATKXs+GVX8cmk75J+SGLFmhXcaUcyV88TIthzzDzuHVNYTGExhcUUFlNYTGExhcUUFlNsWf7P7Do4lcSk+lESeyGYWdkPPD1TjFVdZmXWmr9NupzEGu3kUtLbSXeS7g+JGZgdxOeTvGMKiyksprCYwmIKiyksprCYwmIkSZIkSZIkSZIkSZIkSZIkSZIkSZIkSZL03P0LMTmIwV9j1hUAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=280x280>"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"train set: \", train_ds)\n",
    "print(\"test set: \", test_ds)\n",
    "\n",
    "#show image and corresponding label\n",
    "image_0 = Image.fromarray(test_ds[\"image\"][0])\n",
    "print(\"\\n label of image: \", test_ds[\"label\"][0])\n",
    "image_0.resize((image_0.width * 10, image_0.height * 10), Image.NEAREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels\n",
    "\n",
    "#0\tT-shirt/top\n",
    "#1\tTrouser\n",
    "#2\tPullover\n",
    "#3\tDress\n",
    "#4\tCoat\n",
    "#5\tSandal\n",
    "#6\tShirt\n",
    "#7\tSneaker\n",
    "#8\tBag\n",
    "#9\tAnkle boot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train and test data up into separate arrays for image and label\n",
    "train_images = train_ds['image']\n",
    "train_labels = train_ds['label']\n",
    "test_images = test_ds['image']\n",
    "test_labels = test_ds['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize image values from [0,255] to [0,1]\n",
    "train_images =  (train_images.astype('float32')) * 1/255\n",
    "test_images  =  (test_images.astype('float32')) * 1/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(count samples, width, height):  (60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# image: 28 x 28 --> vector: 1 x 784\n",
    "print(\"(count samples, width, height): \", train_images.shape)\n",
    "input_dim = train_images.shape[1] * train_images.shape[2]\n",
    "\n",
    "train_images_flat = train_images.reshape(train_images.shape[0], input_dim)\n",
    "test_images_flat  = test_images.reshape(test_images.shape[0] , input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding, i.e. \n",
    "# Number 3 -> [0,0,0,1,0,0,0,0,0]\n",
    "train_labels = keras.utils.to_categorical(train_labels, 10) \n",
    "test_labels  = keras.utils.to_categorical(test_labels, 10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "#architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation = \"relu\", input_dim = input_dim))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(512, activation = \"relu\"))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(10, activation = \"softmax\")) #get probability values\n",
    "\n",
    "# defining optimizer, compile model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer = SGD(learning_rate=0.01),\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.5311 - loss: 1.3974\n",
      "Epoch 2/10\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.7579 - loss: 0.7085\n",
      "Epoch 3/10\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.7909 - loss: 0.6097\n",
      "Epoch 4/10\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.8065 - loss: 0.5614\n",
      "Epoch 5/10\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.8201 - loss: 0.5263\n",
      "Epoch 6/10\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.8266 - loss: 0.5031\n",
      "Epoch 7/10\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.8307 - loss: 0.4847\n",
      "Epoch 8/10\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.8377 - loss: 0.4618\n",
      "Epoch 9/10\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.8399 - loss: 0.4574\n",
      "Epoch 10/10\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.8464 - loss: 0.4396\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x15249bc93f0>"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images_flat, train_labels,\n",
    "          epochs = 10, \n",
    "          batch_size = 100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8528 - loss: 0.4215\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_images_flat, test_labels, batch_size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASwAAAEsCAAAAABcFtGpAAAEwElEQVR4nO3du45WVRjG8ZnxE1EUMZ5QE9FCjRhjZfAQOyO9sbGVhgvwJmy9ERMLbbgCG4mViY1GjIEong+jAnbreYo3+firyRD8/6o3i2/2bJ5ZxX5nrbVnZ0eSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJB2w3YO+gf/SLau6msFr0ydvS7mf8omUn01ftfcP7+t/ybAAwwIMCzAswLAAwwIMCzAswLCAzUHfQNkdy3Quj2TwxZQfpvzlur/V/jj6esp3pn93ZgGGBRgWYFiAYQGGBRgWYFiAYQE30hN8uToNvpLyVMqHU7573dd/IOXplD9t+SpnFmBYgGEBhgUYFmBYgGEBhgUYFmBYwI3U7mR71c5fKZ9f1dMZvJiyNlW9t6rLGbw95RerujeDR1Ne2HKDzizAsADDAgwLMCzAsADDAgwLMCzAsICDb3fy46oe50jKN1ZVm6oOp7wrZXZ17U2DO8+s6ssMfpdyWxjOLMCwAMMCDAswLMCwAMMCDAswLMCwgH/d7lQ3Uefc96bBKmsh58p01bMps5DzewZPpDw8fbSuX/viclzljwzW6k6dyU+/VWdcnFmAYQGGBRgWYFiAYQGGBRgWYFgAeYIfH9bH11PNx0m2Pba/mfJ4yo9XVfd6LOW3KbMr674M1oJG3UDUdLkjZXZ9nR8/qm0MCzAswLAAwwIMCzAswLAAwwIMCyDtztjZ7I1l2plr02B7a1VPZrD2T+XkSLVbdcbkq5TpbKrd+jVl1jbmZZaSM/nnM+jMAgwLMCzAsADDAgwLMCzAsADDAgwLmNudOcJqDNIuVGMxLumUei1YvZg3nUv9RZI7U2bTVB2Zr/1VdVe1OhPVZO1Pg7X/qv4DL0+XcmYBhgUYFmBYgGEBhgUYFmBYgGEBm3HT1LZn8fm3/PenfGxVT2XwoZT1BP7jqo5lsI493LqqOgBRN3hi+uj3Gfxz+qqaI7+lrCzy5uCcQndmEYYFGBZgWIBhAYYFGBZgWIBhAYYFbMZNUw+mrG7iyFTWTqnHU2bpoLqNn1PWz+ju6VL1Kq1cqnZa1au0DqX8erhor2LkVVm1IHJPylq7yHmYWiZxZgGGBRgWYFiAYQGGBRgWYFiAYQGGBfT+rFdXVTupqvGo1Zusg1wZP5rFkWos6kh9HQfJmk29uLd+hrlALb5UY1JN1A+rqj/2OKpvVQtF1W/lrup/5cwCDAswLMCwAMMCDAswLMCwAMMCDAvYvJb6zKo+zWBWTPpPwyfj2pZW7UjamfqiWoepHiNH5ufT9flo9qp151QrUSenbzVOh2qXavWn3kecJurSlktpZliAYQGGBRgWYFiAYQGGBRgWsPko9QurejaD48nq+i1+PaFfnsosIvRjdT2sZ/9TnVypx+qcTKkzMM+l/CTl56vK2ksfYhkP0dSCRL2KK4dk6nXDzizAsADDAgwLMCzAsADDAgwLMCzAsIDdbR+o/VWnUqYzeSmDtX8rPUqdYZnf0ZsFiWqXasXk3Ko+yGAtLYzeT/loym9WVU1aldX55LzL2xl0ZgGGBRgWYFiAYQGGBRgWYFiAYQGGJUmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJN2k/gbLOYjXQz4AnAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=300x300>"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = 0\n",
    "img = Image.fromarray((test_images[sample] * 255).astype(\"uint8\"))\n",
    "\n",
    "print(model.predict((train_images[sample]*255).reshape(1, 784)))\n",
    "img.resize((300,300), Image.NEAREST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn = Sequential()\n",
    "\n",
    "#Convolutional Layers\n",
    "model_cnn.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model_cnn.add(MaxPooling2D((2, 2)))\n",
    "model_cnn.add(Conv2D(64, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model_cnn.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "#MLP (same one as before (except input))\n",
    "model_cnn.add(Flatten())\n",
    "model_cnn.add(Dense(64, activation = \"relu\"))\n",
    "model_cnn.add(Dropout(0.25))\n",
    "model_cnn.add(Dense(10, activation = \"softmax\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 21ms/step - accuracy: 0.3177 - loss: 1.9125\n",
      "Epoch 2/10\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 21ms/step - accuracy: 0.6566 - loss: 0.9371\n",
      "Epoch 3/10\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 25ms/step - accuracy: 0.7125 - loss: 0.7747\n",
      "Epoch 4/10\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 21ms/step - accuracy: 0.7367 - loss: 0.7101\n",
      "Epoch 5/10\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 22ms/step - accuracy: 0.7575 - loss: 0.6549\n",
      "Epoch 6/10\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 22ms/step - accuracy: 0.7706 - loss: 0.6206\n",
      "Epoch 7/10\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 21ms/step - accuracy: 0.7780 - loss: 0.5947\n",
      "Epoch 8/10\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 21ms/step - accuracy: 0.7881 - loss: 0.5706\n",
      "Epoch 9/10\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 22ms/step - accuracy: 0.7917 - loss: 0.5605\n",
      "Epoch 10/10\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 22ms/step - accuracy: 0.7981 - loss: 0.5535\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x152363eae00>"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# defining optimizer, compile model\n",
    "model_cnn.compile(loss='categorical_crossentropy',\n",
    "              optimizer = SGD(learning_rate=0.01),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "model_cnn.fit(train_images, train_labels, \n",
    "              epochs=10, \n",
    "              batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.8327 - loss: 0.4600\n"
     ]
    }
   ],
   "source": [
    "score = model_cnn.evaluate(test_images, test_labels, batch_size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 0\n",
    "img = Image.fromarray((test_images[sample] * 255).astype(\"uint8\"))\n",
    "\n",
    "print(model_cnn.predict((train_images[sample]*255).reshape(1, 784)))\n",
    "img.resize((300,300), Image.NEAREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = train_ds['image'][sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = 0\n",
    "\n",
    "resized_image = img.resize((img.width * 10, img.height * 10), Image.NEAREST)\n",
    "img1 = np.expand_dims(np.array(img), axis=-1)\n",
    "img1.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Conv2D.call().\n\n\u001b[1mNegative dimension size caused by subtracting 3 from 1 for '{{node sequential_8_1/conv2d_15_1/convolution}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](sequential_8_1/ExpandDims, sequential_8_1/conv2d_15_1/convolution/ReadVariableOp)' with input shapes: [28,28,1,1], [3,3,1,32].\u001b[0m\n\nArguments received by Conv2D.call():\n  • inputs=tf.Tensor(shape=(28, 28, 1, 1), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel_cnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg1\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      2\u001b[0m resized_image\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling Conv2D.call().\n\n\u001b[1mNegative dimension size caused by subtracting 3 from 1 for '{{node sequential_8_1/conv2d_15_1/convolution}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](sequential_8_1/ExpandDims, sequential_8_1/conv2d_15_1/convolution/ReadVariableOp)' with input shapes: [28,28,1,1], [3,3,1,32].\u001b[0m\n\nArguments received by Conv2D.call():\n  • inputs=tf.Tensor(shape=(28, 28, 1, 1), dtype=float32)"
     ]
    }
   ],
   "source": [
    "print(model_cnn.predict(img1))\n",
    "resized_image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
